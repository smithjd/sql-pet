# Introduction to DBMS queries (11)

These are the "regular" packages are used in this chapter:
```{r setup with standard packages, echo=TRUE, message=FALSE, warning=FALSE}
library(tidyverse)
library(DBI)
library(RPostgres)
library(dbplyr)
library(glue)
require(knitr)
library(bookdown)
library(sqlpetr)
```
In addition we will demonstrate these
```{r Additional packages}
library(skimr)
library(janitor)
```

Assume that the Docker container with PostgreSQL and the dvdrental database are ready to go. 
```{r}
sp_docker_start("sql-pet")
```
Connect to the database:
```{r}
con <- sp_get_postgres_connection(user = Sys.getenv("DEFAULT_POSTGRES_USER_NAME"),
                         password = Sys.getenv("DEFAULT_POSTGRES_PASSWORD"),
                         dbname = "dvdrental",
                         seconds_to_test = 10)

```

## Getting data from the database

As we show later on, the database serves as a store of data and as an engine for sub-setting, joining, and doing computation.  We begin with simple extraction, or "downloading" data.

### Finding out what's there

We've already seen the simplest way of getting a list of tables in a database with `DBI` functions that  list tables and fields.  Here are the (public) tables in the database:
```{r}
DBI::dbListTables(con)
```
Here are the fields (or columns or variables) in one specific table:
```{r}
DBI::dbListFields(con, "rental")
```

Later on we'll discuss how to get more extensive data about each table and column from the database's own store of metadata.

### Getting an entire table from the database

There are many different methods of getting data from a DBMS, and we'll explore the different ways of controlling each one of them.

`DBI::dbReadTable` will download an entire table into an R [tibble](https://tibble.tidyverse.org/).
```{r}

rental_tibble <- DBI::dbReadTable(con, "rental") 
str(rental_tibble)
```
That's very simple, but if the table is large it may not be a good idea, since R is designed to keep the entire table in memory.

### Referencing a table for subsequent use

The `dplyr::tbl` function gives us more control over access to a table.  It creates  a connection object that might look like a data frame but it's actually an list object that `dplyr` uses for constructing queries and retrieving data from the DBMS.  
```{r}
rental_table <- dplyr::tbl(con, "rental")
```
Consider the structure of the connection object:
```{r}
str(rental_table)
```
Notice that the first list contains the source connection information.  Among other things it contains a list of variables in the table:
```{r}
rental_table$ops$vars
```
But because of lazy loading, R has not retrieved any actual data from the DBMS when you reference the `rental_table` object with `str`.  Because R is lazy and smart, it retrieves data as late as possible and only retrieves a certain number of rows.  This is a key paradigm shift for those new to working databases using R and `dplyr`.  

We can trigger data retrieval in several ways. The `head` function, for example, triggers a query and prints its results.  And R *assumes* a `print` function when it finds an object's name on the command line.  By default, these two functions print a different number of rows: `head` defaults to 6 rows and an implied `print` defaults to 10.
```{r}
rental_table %>% head
rental_table
```

In the code block below, we see that `nrows` is like `str` in that it does not trigger a query to the dbms: it just returns NA. See [Controlling number of rows returned] for  how to tell R to quit being lazy, get to work, and return all the rows.

```{r}
nrow(rental_table)
```


### Sub-setting variables

A table in the dbms may not only have many more rows than you want and also many more columns.  The `select` command controls which columns are retrieved.
```{r}
rental_table %>% select(rental_date, return_date) %>% head
```

We won't discuss `dplyr` methods for sub-setting variables, deriving new ones, or sub-setting rows based on the values found in the table because they are covered well in other places, including:

  * Comprehensive reference: [https://dplyr.tidyverse.org/](https://dplyr.tidyverse.org/)
  * Good tutorial: [https://suzan.rbind.io/tags/dplyr/](https://suzan.rbind.io/tags/dplyr/) 

In practice we find that, **renaming columns** is often quite important because the names in an SQL database might not meet your needs as an analyst.  In "the wild" you will find names that are ambiguous or overly specified, with spaces in them, and other problems that will make them difficult to use in R.  It is good practice to do whatever renaming you are going to do in a predictablel place like at the top of your code.  The names in the `dvdrental` database are simple and clear, but if they were not, you might rename them for subsequent use in this way:

```{r}
renamed_rental_table <- dplyr::tbl(con, "rental") %>% 
  rename(rental_id_number = rental_id, inventory_id_number = inventory_id)

renamed_rental_table %>% 
  select(rental_id_number, rental_date, inventory_id_number) %>% 
  head()
```

In the case of the `dvdrental` database, the column names are short and very friendly.  But in the wild you will find many tables with difficult to use names.  The `janitor::clean_names` is a good first try for making column names easier to handle.
```{r}

rental_tibble %>% janitor::clean_names() %>% as_tibble()

```


### Controlling number of rows returned

The `collect` function triggers the creation of a tibble and controls the number of rows that the DBMS sends to R.
```{r}
rental_table %>% collect(n = 3) %>% head
```
In this case the `collect` function triggers the execution of a query that counts the number of records in the table by `staff_id`:
```{r}
rental_table %>% 
  count(staff_id) %>% 
  collect()
```

The `collect` function affects how much is downloaded, not how many rows the DBMS needs to process the query. This query processes all of the rows in the table but only displays one row of output.
```{r}
rental_table %>% 
  count(staff_id) %>% 
  collect(n = 1)
```

### Random rows from the dbms

When the dbms contains many rows, a sample of the data may be plenty for your purposes.  Although `dplyr` has nice functions to sample a data frame that's already in R (e.g., the `sample_n` and `sample_frac` functions), to get a sample from the dbms we have to use `dbGetQuery` to send native SQL to the database. To peak ahead, here is one example of a query that retrieves 20 rows from a 1% sample:

```{r}
one_percent_sample <- DBI::dbGetQuery(con,
  "SELECT rental_id, rental_date, inventory_id, customer_id FROM rental TABLESAMPLE SYSTEM(1) LIMIT 20;
  ")

one_percent_sample

```

### Examining `dplyr`'s SQL query and re-using SQL code

The `show_query` function shows how `dplyr` is translating your query to the dialect of the target dbms:
```{r}
rental_table %>% 
  count(staff_id) %>% 
  show_query()
```
Here is an extensive discussion of how `dplyr` code is translated into SQL:

* [https://dbplyr.tidyverse.org/articles/sql-translation.html](https://dbplyr.tidyverse.org/articles/sql-translation.html) 

The SQL code can submit the same query directly to the DBMS with the `DBI::dbGetQuery` function:
```{r}
DBI::dbGetQuery(con,
  'SELECT "staff_id", COUNT(*) AS "n"
   FROM "rental"
   GROUP BY "staff_id";
  ')

```
<<smy We haven't investigated this, but it looks like `dplyr` collect() function triggers a call simmilar to the dbGetQuery call above.  The default `dplyr` behavior looks like dbSendQuery() and dbFetch() model is used.>>

When you create a report to run repeatedly, you might want to put that query into R markdown.  That way you can also execute that SQL code in a chunk with the following header:

  {`sql, connection=con, output.var = "query_results"`}

```{sql, connection=con, output.var = "query_results"}
SELECT "staff_id", COUNT(*) AS "n"
FROM "rental"
GROUP BY "staff_id";
```
Rmarkdown stored that query result in a tibble:
```{r}
query_results
```

## Investigating a single table with R

Dealing with a large, complex database highlights the utility of specific tools in R.  We include brief examples that we find to be handy:

  + Base R structure: `str`
  + printing out some of the data: `datatable`, `kable`, and `View`
  + summary statistics: `summary`
  + `glimpse` in the   `tibble` package, which is included in the `tidyverse`
  + `skim` in the `skimr` package

### `str` - a base package workhorse

`str` is a workhorse function that lists variables, their type and a sample of the first few variable values.
```{r}
str(rental_tibble)
```

### Always just **look** at your data with `head`, `View`, or `kable`

There is no substitute for looking at your data and R provides several ways to just browse it.  The `head` function controls the number of rows that are displayed.  Note that tail does not work against a database object.  In every-day practice you would look at more than the default 6 rows, but here we wrap `head` around the data frame: 
```{r}
sp_print_df(head(rental_tibble))
```

### The `summary` function in base

The basic statistics that the base package `summary` provides can serve a unique diagnostic purpose in this context.  For example, the following output shows that `rental_id` is a sequential number from 1 to 16,049 with no gaps.  The same is true of `inventory_id`.  The number of NA's is a good first guess as to the number of dvd's rented out or lost on 2005-09-02 02:35:22.
```{r}
summary(rental_tibble)
```

### The `glimpse` function in the `tibble` package

The `tibble` package's `glimpse` function is a more compact version of `str`:
```{r}
tibble::glimpse(rental_tibble)
```
### The `skim` function in the `skmir` package

The `skimr` package has several functions that make it easy to examine an unknown data frame and assess what it contains. It is also extensible.
```{r}

skim(rental_tibble)

wide_rental_skim <- skim_to_wide(rental_tibble)
```

## Dividing the work between R on your machine and the DBMS

They work together.

### Make the server do as much work as you can

* show_query as a first draft of SQL.  May or may not use SQL code submitted directly.

### Criteria for choosing between `dplyr` and native SQL

**This probably belongs later in the book.**

* performance considerations: first get the right data, then worry about performance
* Trade offs between leaving the data in PostgreSQL vs what's kept in R: 
  + browsing the data
  + larger samples and complete tables
  + using what you know to write efficient queries that do most of the work on the server

### `dplyr` tools

already covered?

### Using `glue` to construct SQL statements

Here is one example:
```{r}

table <- "rental"
vals = c(148, 526)

sql <- glue_sql("
  SELECT * 
  FROM {`table`}",  # glue will automatically paste chunks of SQL together 
  "WHERE customer_id IN ({vals*})
", .con = con)

sql

query <- DBI::dbSendQuery(con, sql)
df <- dbFetch(query)
str(df)
DBI::dbClearResult(query)

```

Several more examples are here: https://glue.tidyverse.org/reference/glue_sql.html 

Where you place the `collect` function matters.
```{r}
dbDisconnect(con)
sp_docker_stop("sql-pet")
```

## Other resources

  * Benjamin S. Baumer, A Grammar for Reproducible and Painless Extract-Transform-Load Operations on Medium Data: [https://arxiv.org/pdf/1708.07073](https://arxiv.org/pdf/1708.07073) 

